{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38742271-5f40-4436-b301-4ca623d441d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import FloatProgress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3833b735-ebd7-46cc-bbb3-e9b1d171cade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.8.1+cu101\n",
      "cuda available: 4\n",
      "cuda current device: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.device_count())\n",
    "print(\"cuda current device:\", torch.cuda.current_device())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cca49d14-1bd3-4e67-9e93-5de12af31536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "device = 2\n",
    "model_name = 'google/pegasus-xsum'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62342585-14a9-4205-8183-79adf917cdca",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  143, 40155,   158, 77085,   151,   896,  1758,   108,   355,   129,\n",
       "          7640,   112,  3832,  6787,   113,  4347, 84464,   116,   111, 11131,\n",
       "         18585,   107, 11869, 17350,   120,  9903,  2603,  6339,  8586,   117,\n",
       "          2096,   109,  2713,   198, 40328, 10339,   194,  1397,   143,   159,\n",
       "          5090, 28942,   211,  4820,   109,   584,   250,  2603,  6339,  8586,\n",
       "           140,  1487,   130,  1758,   113,   109,  1397,   115,  1410,   107,\n",
       "         11869,  3686,   114,  1736,   135, 11235, 13339,   107,  8570,   120,\n",
       "           649,   108,   198, 12580,  1392,  3888,   108, 11235, 13339,   107,\n",
       "           111,  9903,  2603,  6339,  8586,   133,  1159,   146,   112,   696,\n",
       "           782,   122,  1017,   112,  1070,   111,  1443,  1034, 40328, 10339,\n",
       "           131,   424,   496,   143, 40155,   111, 11235, 13339,   107,  8570,\n",
       "           127,   302,  2627,   141,  2247, 11235,   107,   158,   139,  1397,\n",
       "           108, 11692, 13918,  8501, 15650,   115,   109,  1560,   868,   113,\n",
       "           109,  2107, 13515,   108,   117,   309,   323,   118,  1131,   124,\n",
       "          1185,  7801,  3039,   168,   131,   116,   109,   211, 15808,  1397,\n",
       "         57013,   279,   109,   205,   785,  2912, 16167,   107,  8501, 15650,\n",
       "           138,  1699, 14228,   115,   198, 67271,  2294,   107, 19589,   151,\n",
       "         13727,   113,  4465,   745,   640,   165,  1051,  8618,  3499,   222,\n",
       "           109,  8798,   108, 11235,   138,   217,   112,   258,   647,   177,\n",
       "           118,   109,  1758,   131,   116,  2519,   107,     1]],\n",
       "       device='cuda:2'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]], device='cuda:2')}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text = [\n",
    "    # \"\"\" PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\"\",\n",
    "    \"\"\"(CNN)Wanted: film director, must be eager to shoot footage of golden lassos and invisible jets. CNN confirms that Michelle MacLaren is leaving the upcoming \"Wonder Woman\" movie (The Hollywood Reporter first broke the story). MacLaren was announced as director of the movie in November. CNN obtained a statement from Warner Bros. Pictures that says, \"Given creative differences, Warner Bros. and Michelle MacLaren have decided not to move forward with plans to develop and direct \\'Wonder Woman\\' together.\" (CNN and Warner Bros. Pictures are both owned by Time Warner.) The movie, starring Gal Gadot in the title role of the Amazon princess, is still set for release on June 23, 2017. It\\'s the first theatrical movie centering around the most popular female superhero. Gadot will appear beforehand in \"Batman v. Superman: Dawn of Justice,\" due out March 25, 2016. In the meantime, Warner will need to find someone new for the director\\'s chair.\"\"\"\n",
    "]\n",
    "batch = tokenizer(src_text, truncation=True, padding='longest', return_tensors=\"pt\").to(device)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27899870-e901-4b6c-a543-962b170f8e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CudaDeviceProperties(name='GeForce GTX 1080 Ti', major=6, minor=1, total_memory=11178MB, multi_processor_count=28)\n",
      "0.0 MiB\n",
      "0.0 MiB\n",
      "0.0 MiB\n",
      "_CudaDeviceProperties(name='GeForce GTX 1080 Ti', major=6, minor=1, total_memory=11178MB, multi_processor_count=28)\n",
      "0.0 MiB\n",
      "0.0 MiB\n",
      "0.0 MiB\n",
      "_CudaDeviceProperties(name='GeForce GTX 1080 Ti', major=6, minor=1, total_memory=11178MB, multi_processor_count=28)\n",
      "2290.089984 MiB\n",
      "2280.009216 MiB\n",
      "10.080768 MiB\n",
      "_CudaDeviceProperties(name='GeForce GT 710', major=3, minor=5, total_memory=1993MB, multi_processor_count=1)\n",
      "0.0 MiB\n",
      "0.0 MiB\n",
      "0.0 MiB\n"
     ]
    }
   ],
   "source": [
    "cuda_count = torch.cuda.device_count()\n",
    "for i in range(cuda_count):\n",
    "    print(torch.cuda.get_device_properties(i))\n",
    "    print(torch.cuda.memory_reserved(i)/1000000, \"MiB\")\n",
    "    print(torch.cuda.memory_allocated(i)/1000000, \"MiB\")\n",
    "    print((torch.cuda.memory_reserved(i)-torch.cuda.memory_allocated(i))/1000000, \"MiB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabe0e2f-d2a2-4239-8d69-6c825a6fc996",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
